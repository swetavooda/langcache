{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentenceFeature Extractor bottleneck analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and observations from the workflow profiling\n",
    "One of the major bottle neck observed during the workflow profiling was the overhead for SentenceFeature Extractor (i.e., function used to calculate the feature embedding value)\n",
    "\n",
    "### Problem:\n",
    "The curent implementation calculated the feature embedding for every entry in the cache table to find the most similar cache entry from the database. It is to be noted that the SentenceFeature function has a significant overhead since it needs to compute the embedding for a given sentence(text). Computing this value each time is going to negatively impact performance of the cache library. Therefore, it is necessary to find solutions to avoid this overhead and improve the performance of the SentenceFeature extraction\n",
    "\n",
    "### Proposed solution\n",
    "Instead of computing the embeddings each time on database search, we can precompute the embedding and store the feature in a separate column of the cache database to avoid the repeated overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get OpenAI key if needed\n",
    "from openai import OpenAIError\n",
    "import os\n",
    "\n",
    "try:\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "except OpenAIError:\n",
    "    api_key = str(input(\"ðŸ”‘ Enter your OpenAI key: \"))\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = str(input(\"ðŸ”‘ Enter your OpenAI key: \"))\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance analysis with precomputed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-26-2023 03:48:59 WARNING[executor_utils:executor_utils.py:handle_if_not_exists:0094] Table: embeddings already exists\n",
      "/Users/swetavooda/workspace/langCache/langcache/langcache-venv/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:157: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pair 101\n",
      "Total similar pair 36\n",
      "Total time taken after using Precomputed Embeddings for Similarity  66.65347003936768\n",
      "Precision 0.5666666666666667\n",
      "Recall 0.9444444444444444\n",
      "TP FN FP 34 2 26\n"
     ]
    }
   ],
   "source": [
    "from large_test_optimized import test_optimized_largetest\n",
    "\n",
    "test_optimized_largetest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance analysis without precomputed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swetavooda/workspace/langCache/langcache/langcache-venv/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:157: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pair 101\n",
      "Total similar pair 36\n",
      "Total time taken 168.68619990348816\n",
      "Precision 0.5666666666666667\n",
      "Recall 0.9444444444444444\n",
      "TP FN FP 34 2 26\n"
     ]
    }
   ],
   "source": [
    "from large_test_old import test_largetest\n",
    "\n",
    "test_largetest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparision\n",
    "\n",
    "**With precomputed embeddings** Total time taken for cache.get() for 100 entries = 66.65347003936768\n",
    "\n",
    "**With precomputed embeddings** Total time taken for cache.get() for 100 entries = 168.68619990348816\n",
    "\n",
    "By using precomputed embeddings, the performance of the cache.get() function can be improved significantly as detailed in below:\n",
    "#### Speedup: \n",
    "The optimized code is approximately **2.53 times faster** than the unoptimized code.\n",
    "#### Relative Improvement: \n",
    "The optimized code is approximately **60.49% faster** than the unoptimized code.\n",
    "#### Throughput:\n",
    "Before optimization: Throughput (per 100 entries) = 100 / 168.68 â‰ˆ 0.593 operations per 100 entries per second.\n",
    "\n",
    "After optimization: Throughput (per 100 entries) = 100 / 66.65 â‰ˆ 1.499 operations per 100 entries per second.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langcache-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
